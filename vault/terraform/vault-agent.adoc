== Kubernetes PKI management using Hashicorp Vault

Once Vault is up and running and provisioned with the terraform config in this folder, the following script can be used to create the necessary environment and authentication tokens for `vault-agent` to start issuing and rotating certificates for the Kubernetes cluster.

Make sure to first `vault login` with the root token and then run the following script:

[source,console]
----
CLUSTER_NAME=<name of cluster>
kubernetes_id=$(vault read --field=role_id auth/approle/role/$CLUSTER_NAME-node-kubernetes/role-id)
kubernetes_secret=$(vault write --field=secret_id -f auth/approle/role/$CLUSTER_NAME-node-kubernetes/secret-id)
etcd_id=$(vault read --field=role_id auth/approle/role/$CLUSTER_NAME-node-etcd/role-id)
etcd_secret=$(vault write --field=secret_id -f auth/approle/role/$CLUSTER_NAME-node-etcd/secret-id)
frontproxy_id=$(vault read --field=role_id auth/approle/role/$CLUSTER_NAME-node-front-proxy/role-id)
frontproxy_secret=$(vault write --field=secret_id -f auth/approle/role/$CLUSTER_NAME-node-front-proxy/secret-id)
mkdir -p /var/lib/secrets/{kubernetes,etcd,coredns,front-proxy}
echo $kubernetes_id > /var/lib/secrets/kubernetes/vault_agent_role_id
echo $kubernetes_secret > /var/lib/secrets/kubernetes/vault_agent_secret_id
echo $frontproxy_id > /var/lib/secrets/front-proxy/vault_agent_role_id
echo $frontproxy_secret > /var/lib/secrets/front-proxy/vault_agent_secret_id
echo $kubernetes_id > /var/lib/secrets/coredns/vault_agent_role_id
echo $kubernetes_secret > /var/lib/secrets/coredns/vault_agent_secret_id
echo $etcd_id > /var/lib/secrets/etcd/vault_agent_role_id
echo $etcd_secret > /var/lib/secrets/etcd/vault_agent_secret_id
systemctl restart vault-agent-{coredns,etcd,kubernetes-worker,kubernetes-control-plane}
----

Once complete, make sure to `rm ~/.vault-token` to remove the root token from the system.

== Patroni vault-agent provisioning

Patroni uses vault-agent to obtain etcd client certificates for communicating with the etcd cluster. After running `terraform apply` for the `live/235` config, provision the Patroni vault-agent credentials on each node:

[source,console]
----
CLUSTER_NAME=<name of cluster>
patroni_id=$(vault read --field=role_id auth/approle/role/$CLUSTER_NAME-node-patroni/role-id)
patroni_secret=$(vault write --field=secret_id -f auth/approle/role/$CLUSTER_NAME-node-patroni/secret-id)
mkdir -p /var/lib/secrets/patroni
chown patroni:patroni /var/lib/secrets/patroni
echo $patroni_id > /var/lib/secrets/patroni/vault_agent_role_id
echo $patroni_secret > /var/lib/secrets/patroni/vault_agent_secret_id
chown patroni:patroni /var/lib/secrets/patroni/vault_agent_{role,secret}_id
systemctl restart vault-agent-patroni
----

NOTE: Generate a separate `secret_id` for each node (run the `vault write -f` command once per node).

== Build the admin kubeconfig

To bootstrap OIDC authenticaiton, we first need to gain admin access to the cluster to create the proper `ClusterRoleBinding` for the OIDC account. We will get admin by building an admin kubeconfig file using the following script:

[source,bash]
----
#!/usr/bin/env bash

CLUSTER_NAME="k8s-305-1700-1"
ADMIN_USER="admin"
CONTEXT_NAME="admin@$CLUSTER_NAME"
SERVER="https://k8s.305-1700.tdude.co"
CERT_DIR="/var/lib/secrets/kubernetes"

kubectl config set-cluster $CLUSTER_NAME \
    --certificate-authority=${CERT_DIR}/kubernetes-ca.pem \
    --embed-certs=true \
    --server=$SERVER \
    --kubeconfig=admin.kubeconfig

kubectl config set-credentials $ADMIN_USER \
    --client-certificate=${CERT_DIR}/admin.pem \
    --client-key=${CERT_DIR}/admin-key.pem \
    --embed-certs=true \
    --kubeconfig=admin.kubeconfig

kubectl config set-context $CONTEXT_NAME \
    --cluster=$CLUSTER_NAME \
    --user=$ADMIN_USER \
    --kubeconfig=admin.kubeconfig

kubectl config use-context $CONTEXT_NAME --kubeconfig=admin.kubeconfig

echo "Admin kubeconfig created as admin.kubeconfig"
----