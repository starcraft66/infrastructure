= 235 Kubernetes cluster
// Only set imagesdir if the file hasn't been included by one with a different imagesdir
ifndef::imagesdir[:imagesdir: ../.gitlab/images]

== Overview

[.text-justify]
The 235 kubernetes cluster (referred to as `k8s-235`) is my only personal kubernetes cluster at the moment. It consists of 3 consolidated master, worker and etcd nodes. Etcd is the backing data store for kubernetes. The cluster is deployed on 3 physical (bare metal) servers running NixOS, these hosts are provisioned by the Nix expressions and modules located in the `nixos` folder at the root of the repo. The kubernetes cluster deployed using a combination of the kubernetes module included in upstream nixpkgs and a custom module that I wrote specific to my infrastructure. Most importantly, it handles end-to-end mutual TLS certification of all cluster components using Hashicorp Vault and vault-agent.

== Provisioning

[.text-justify]
The physical servers providing compute for the kubernetes worker and master nodes are provisioned by Nix. The flake in this repo contains NixOS configurations for the pysical servers `soarin`, `sassaflash` and `stormfeather`.

.Procedure to install a new physical NixOS server
[%collapsible]
====
TODO: document
====

.Procedure to initialize, backup and restore the etcd database
[%collapsible]
====
TODO: document
====

== Components

[.text-justify]
The core components of the kubernetes cluster include the control-plane itself, etcd, the Cilium CNI networking plugin and the democratic-csi CSI storage plugin. Cilium is installed at cluster provisioning time using a manual invocation and democratic-csi is later installed by ArgoCD. Cilium can then be further upgraded by ArgoCD.

=== üêù Cilium

I used to use the Calico CNI but recently migrated to Cilium. The semantics are mostly the same when it comes to the BGP peering architecture, I hope to post updated documentation soon.

=== üêô ArgoCD

[.text-justify]
ArgoCD is an central part of my cluster. Once the initial control-plane for the cluster is deployed by ansible, the only application that must be installed manually into the cluster is ArgoCD. Once installed, it will take over the lifecycle of all of the applications in this repository to be deployed into the cluster.
ArgoCD deploys everything in the `k8s-235/apps` folder including itself. A commit webhook in GitLab will cause ArgoCD to sync the repository on every commit. Authentication to ArgoCD is done through GitLab's SSO.

[.text-justify]
To deploy ArgoCD, go into the `k8s-235/apps/argocd` and run `kustomize build --enable-alpha-plugins overlays/prod | kubectl apply -f -`. ArgoCD will become available at https://gitops.tdude.co.

==== Kustomize

[.text-justify]
Kustomize touts itself to be a template-free solution to manage application deployments on kubernetes in stark contrast to the popular Helm package manager. All of the manifests in `k8s-235/apps` are assembled with kustomize before being applied. ArgoCD runs kustomize which builds the manifests and applies them to the cluster.

==== SOPS

ArgoCD is capable of decrypting secrets in this repo through the https://github.com/viaduct-ai/kustomize-sops[kustomize-sops] plugin for kustomize. To do this, an `initContainer` installs the plugin into ArgoCD's Pod and also imports a GPG private key ArgoCD uses to decrypt secrets. All sops-encrypted secrets you expect ArgoCD to be able to decrypt must be encrypted with ArgoCD's public key.

.Procedure to generate a GPG key for use with ArgoCD
[%collapsible]
====
Procedure goes here...
====

=== üî• Prometheus

[.text-justify]
Cluster-wide metrics monitoring in my cluster is performed by prometheus. More specifically, I use ArgoCD to deploy https://git.tdude.co/tristan/kube-prometheus-k8s-235/-/tree/master/[my personal configuration] of the https://github.com/prometheus-operator/kube-prometheus/[kube-prometheus] project. Like everything else, it is deployed by ArgoCD and configured to scrape ``ServiceMonitor``s and ``PodMonitor``s in all namespaces, along with cluster-wide internal metrics. The web interface for prometheus is reachable at https://prometheus.monitoring.tdude.co and the one for Grafana at https://monitoring.tdude.co. Grafana is configured for SSO authentication with GitLab.

[.text-justify]
Additionally, `kube-prometheus` also deploys alertmanager and a handful of useful alerting rules. I have configured it to notify me on Discord and on Matrix when alerts are firing using https://github.com/benjojo/alertmanager-discord/[alert-manager-discord] and https://github.com/jaywink/matrix-alertmanager/[matrix-alertmanager]. The web interface for alertmanager is available at https://alertmanager.tdude.co.

=== üõ©Ô∏è Traefik

[.text-justify]
The idiomatic way to expose http services in kubernetes is via an ingress controller, a fancy word for a reverse proxy that does some service discovery. For this task, I use https://traefik.io/traefik/[Traefik] for no particular reason other than having used it in the past and having liked it.

==== Single Sign-On

[.text-justify]
Traefik supports a host of interesting features like its extensive middleware system. I make extensive use of the "Forward Auth" middleware to protect services I don't want exposed to the public behind an OpenID Connect SSO login via https://github.com/oauth2-proxy/oauth2-proxy/[oauth2-proxy]. SSO protection can be added to any ingress by setting the `traefik.ingress.kubernetes.io/router.middlewares` label on it to `traefik-forward-auth@kubernetescrd`.

==== TLS

[.text-justify]
Traefik is configured to accept TLS 1.2 and 1.3 connections only and sets strict-transport security headers unconditionally. While it is capable of obtaining TLS certificates on its own via ACME, I choose to disable that functionality and provision TLS certificates with https://cert-manager.io/[cert-manager] instead because it is much more flexible. Setting the `cert-manager.io/cluster-issuer` label to `"letsencrypt-prod"` on any ingress will cause cert-manager to provision a certificate matching the domains on that ingress. Cert-manager is configured to obtain certificate from the Let's Encrypt staging and production environments via the DNS-01 challenge using Cloudflare's API.

=== üíæ Democratic-csi

[.text-justify]
No kubernetes cluster is complete without persistent storage for stateful applications. My hypervisors aren't particularly interesting in terms of storage, but I do have a NAS with plenty of storage so let's use that! I use the https://github.com/democratic-csi/democratic-csi/[democratic-csi] CSI storage driver in my cluster to provide network programmatic access to network storage on my NAS to my kubernetes nodes. This works by making calls to the TrueNAS http api to provision new ZFS datasets or ZVOLs and exporting it over NFS or iSCSI depending on the type of share. The configuration in `k8s-235/apps/democratic-csi` defines 3 distinct ``StorageClass``es for different purposes:

- `freenas-nfs-csi`
- `freenas-iscsi-csi`
- `truenas-nfs-spitfire-fast`

[.text-justify]
`freenas-nfs-csi` provisions HDD-backed ZFS datasets to be shared over NFS. `freenas-iscsi-csi` provisions HDD-backed ZFS datasets to be shared over iSCSI, this is usually required for anything that uses an SQLite database with the WAL enabled since locking does not work over NFS. `truenas-nfs-spitfire-fast` provisions an NVMe SSD-backed ZFS dataset to be shared over NFS.

==== Configuration

[.text-justify]
To in order for democratic-csi to do its work, we need to supply it with a TrueNAS API key and an SSH private key to log into TrueNAS as root to run some ZFS commands (this might not be required in the future).

.Procedure to add an SSH key to TrueNAS root
[%collapsible]
====
Procedure goes here...
====

.Procedure to generate and retrieve a TrueNAS API key
[%collapsible]
====
Procedure goes here...
====